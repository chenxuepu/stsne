% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/neighbors.R, R/stsne.R
\name{stsne_neighbors}
\alias{stsne_neighbors}
\alias{stsne}
\alias{stsne.default}
\alias{stsne.dist}
\alias{stsne.data.frame}
\title{Supervised learning for t-Distributed Stochastic Neighbor Embedding}
\usage{
stsne_neighbors(
  index,
  distance,
  dims = 2,
  perplexity = 30,
  theta = 0.5,
  max_iter = 1000,
  verbose = getOption("verbose", FALSE),
  Y_init = NULL,
  stop_lying_iter = ifelse(is.null(Y_init), 250L, 0L),
  mom_switch_iter = ifelse(is.null(Y_init), 250L, 0L),
  momentum = 0.5,
  final_momentum = 0.8,
  eta = 200,
  exaggeration_factor = 12,
  num_threads = 1,
  weights = rep(1, NROW(distance)),
  r = 2,
  ...
)

stsne(X, ...)

\method{stsne}{default}(
  X,
  dims = 2,
  initial_dims = 50,
  perplexity = 30,
  theta = 0.5,
  check_duplicates = TRUE,
  pca = TRUE,
  partial_pca = FALSE,
  max_iter = 1000,
  verbose = getOption("verbose", FALSE),
  is_distance = FALSE,
  Y_init = NULL,
  pca_center = TRUE,
  pca_scale = FALSE,
  normalize = TRUE,
  stop_lying_iter = ifelse(is.null(Y_init), 250L, 0L),
  mom_switch_iter = ifelse(is.null(Y_init), 250L, 0L),
  momentum = 0.5,
  final_momentum = 0.8,
  eta = 200,
  exaggeration_factor = 12,
  num_threads = 1,
  weights = rep(1, NROW(X)),
  col_weights = NULL,
  only_r = FALSE,
  r = 2,
  ...
)

\method{stsne}{dist}(X, ..., is_distance = TRUE)

\method{stsne}{data.frame}(X, ...)
}
\arguments{
\item{index}{integer matrix; Each row contains the identity of the nearest neighbors for each observation}

\item{distance}{numeric matrix; Each row contains the distance to the nearest neighbors in \code{index} for each observation}

\item{dims}{integer; Output dimensionality (default: 2)}

\item{perplexity}{numeric; Perplexity parameter (should not be bigger than 3 * perplexity < nrow(X) - 1, see details for interpretation)}

\item{theta}{numeric; Speed/accuracy trade-off (increase for less accuracy), set to 0.0 for exact TSNE (default: 0.5)}

\item{max_iter}{integer; Number of iterations (default: 1000)}

\item{verbose}{logical; Whether progress updates should be printed (default: global "verbose" option, or FALSE if that is not set)}

\item{Y_init}{matrix; Initial locations of the objects. If NULL, random initialization will be used (default: NULL). Note that when using this, the initial stage with exaggerated perplexity values and a larger momentum term will be skipped.}

\item{stop_lying_iter}{integer; Iteration after which the perplexities are no longer exaggerated (default: 250, except when Y_init is used, then 0)}

\item{mom_switch_iter}{integer; Iteration after which the final momentum is used (default: 250, except when Y_init is used, then 0)}

\item{momentum}{numeric; Momentum used in the first part of the optimization (default: 0.5)}

\item{final_momentum}{numeric; Momentum used in the final part of the optimization (default: 0.8)}

\item{eta}{numeric; Learning rate (default: 200.0)}

\item{exaggeration_factor}{numeric; Exaggeration factor used to multiply the P matrix in the first part of the optimization (default: 12.0)}

\item{num_threads}{integer; Number of threads to use when using OpenMP, default is 1. Setting to 0 corresponds to detecting and using all available cores}

\item{weights}{vector;the weight for cost}

\item{r}{numeric;a param the can regulation density of point , if r != 2 ,then theta will be set as 0.0. (default: 2)}

\item{...}{Other arguments that can be passed to Rtsne}

\item{X}{matrix; Data matrix (each row is an observation, each column is a variable)}

\item{initial_dims}{integer; the number of dimensions that should be retained in the initial PCA step (default: 50)}

\item{check_duplicates}{logical; Checks whether duplicates are present. It is best to make sure there are no duplicates present and set this option to FALSE, especially for large datasets (default: TRUE)}

\item{pca}{logical; Whether an initial PCA step should be performed (default: TRUE)}

\item{partial_pca}{logical; Whether truncated PCA should be used to calculate principal components (requires the irlba package). This is faster for large input matrices (default: FALSE)}

\item{is_distance}{logical; Indicate whether X is a distance matrix (default: FALSE)}

\item{pca_center}{logical; Should data be centered before pca is applied? (default: TRUE)}

\item{pca_scale}{logical; Should data be scaled before pca is applied? (default: FALSE)}

\item{normalize}{logical; Should data be normalized internally prior to distance calculations with \code{\link{normalize_input}}? (default: TRUE)}

\item{col_weights}{vector;the weight for column. Note that when using this, the pca_scale will Mandatory designation as FALSE}

\item{only_r}{logical;if TRUE will use r code only,and theta will be set as 0.0.if FALSE will use c++ code to do (default: FALSE)}
}
\value{
List with the following elements:
\item{Y}{Matrix containing the new representations for the objects}
\item{N}{Number of objects}
\item{origD}{Original Dimensionality before TSNE (only when \code{X} is a data matrix)}
\item{perplexity}{See above}
\item{theta}{See above}
\item{costs}{The cost for every object after the final iteration}
\item{itercosts}{The total costs (KL-divergence) for all objects in every 50th + the last iteration}
\item{stop_lying_iter}{Iteration after which the perplexities are no longer exaggerated}
\item{mom_switch_iter}{Iteration after which the final momentum is used}
\item{momentum}{Momentum used in the first part of the optimization}
\item{final_momentum}{Momentum used in the final part of the optimization}
\item{eta}{Learning rate}
\item{exaggeration_factor}{Exaggeration factor used to multiply the P matrix in the first part of the optimization}
\item{weights}{the weight for cost}
\item{r}{a param the can regulation density of point}
}
\description{
Supervised learning for t-Distributed Stochastic Neighbor Embedding
}
\section{Methods (by class)}{
\itemize{
\item \code{default}: Default Interface

\item \code{dist}: on given dist object

\item \code{data.frame}: on data.frame
}}

\examples{
iris_unique <- unique(iris) # Remove duplicates
iris_matrix <- as.matrix(iris_unique[,1:4])

# Set a seed if you want reproducible results
set.seed(42)
tsne_out <- stsne(iris_matrix,pca=FALSE,perplexity=30,theta=0.0) # Run TSNE

# Show the objects in the 2D tsne representation
plot(tsne_out$Y,col=iris_unique$Species, asp=1)

# data.frame as input
tsne_out <- stsne(iris_unique,pca=FALSE, theta=0.0)

# Using a dist object
set.seed(42)
tsne_out <- stsne(dist(normalize_input(iris_matrix)), theta=0.0)
plot(tsne_out$Y,col=iris_unique$Species, asp=1)

set.seed(42)
tsne_out <- stsne(as.matrix(dist(normalize_input(iris_matrix))),theta=0.0)
plot(tsne_out$Y,col=iris_unique$Species, asp=1)

# Supplying starting positions (example: continue from earlier embedding)
set.seed(42)
tsne_part1 <- stsne(iris_unique[,1:4], theta=0.0, pca=FALSE, max_iter=350)
tsne_part2 <- stsne(iris_unique[,1:4], theta=0.0, pca=FALSE, max_iter=650, Y_init=tsne_part1$Y)
plot(tsne_part2$Y,col=iris_unique$Species, asp=1)
\dontrun{
# Fast PCA and multicore

tsne_out <- stsne(iris_matrix, theta=0.1, partial_pca = TRUE, initial_dims=3)
tsne_out <- stsne(iris_matrix, theta=0.1, num_threads = 2)
}
}
\references{
Maaten, L. Van Der, 2014. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15, p.3221-3245.

van der Maaten, L.J.P. & Hinton, G.E., 2008. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research, 9, pp.2579-2605.
}
